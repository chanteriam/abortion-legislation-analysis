"""
Implements the TopicModeling class, which applies standard topic modeling
"""

import os
import pickle

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from gensim.models.coherencemodel import CoherenceModel
from gensim.models.ldamodel import LdaModel
from scipy.stats import loguniform, randint

from legislation_analysis.topic_modeling.base_topic_modeling import (
    BaseTopicModeling,
)
from legislation_analysis.utils.constants import (
    MODELED_DATA_PATH,
    TOPIC_MODEL_TRAINING_ITERATIONS,
)


class TopicModeling(BaseTopicModeling):
    """
    TopicModeling class for applying LDA topic modeling techniques to
    pre-tokenized textual data.

    parameters:
        file_path (str): file path to the data to be used for topic modeling.
        save_name (str): file name to save the trained model.
        column (str): name of the column in the DataFrame containing the text to
                      be used for topic modeling.
        max_df (float): maximum document frequency for TF-IDF vectorization.
        min_df (int): minimum document frequency for TF-IDF vectorization.
        topic_ranges (tuple): range of topics to search for the optimal LDA
                              model.
        model_fp (str): file path to a pre-trained LDA model.
    """

    def __init__(
        self,
        file_path: str,
        save_name: str,
        column: str = "text_pos_tags_of_interest",
        max_df: float = 0.8,
        min_df: int = 5,
        topic_ranges: tuple = (2, 30),
        model_fp: str = None,
    ):
        super().__init__(
            file_path=file_path,
            save_name=save_name,
            column=column,
            max_df=max_df,
            min_df=min_df,
            topic_ranges=topic_ranges,
            model_fp=model_fp,
        )

        # saving model parameters
        self.optimal_params = {
            "num_topics": None,
            "alpha": None,
            "eta": None,
            "passes": None,
        }

        # model outputs
        self.topics_by_words_df = None
        self.topics_by_text_df = None

    def compute_coherence(self, model: LdaModel) -> float:
        """
        Computes the coherence score for the given LDA model.

        parameters:
            model (LdaModel/LdaSeqModel): LDA model to compute coherence for.

        returns:
            (float) Coherence score for the given LDA model.
        """
        coherence_model = CoherenceModel(
            model=model,
            texts=self.df["filtered_tokens"],
            dictionary=self.dictionary,
            coherence="c_v",
        )
        return coherence_model.get_coherence()

    def random_search(
        self, iterations: int = TOPIC_MODEL_TRAINING_ITERATIONS
    ) -> None:
        """
        Performs random search to find the optimal LDA model parameters.

        parameters:
            iterations (int): number of iterations for random search.
        """
        best_score = float("-inf")

        for _iter in range(iterations):
            params = {
                "num_topics": randint(
                    self.topic_ranges[0], self.topic_ranges[1]
                ).rvs(),
                "alpha": loguniform(0.0001, 0.5).rvs(),
                "eta": loguniform(0.0001, 0.5).rvs(),
                "passes": randint(10, 50).rvs(),
            }

            print(
                f"""\t(Iteration {_iter+1} of {iterations})
                Trying parameters: {params}"""
            )

            model = LdaModel(
                corpus=self.corpus, id2word=self.dictionary, **params
            )
            score = self.compute_coherence(model)

            print(f"\t\tCoherence Score: {score:.2f}")

            if score > best_score:
                best_score = score
                self.optimal_params = params
                self.lda_model = model

        print(f"Best Score: {best_score}")
        print(f"Best Params: {self.optimal_params}")

        # save model
        self.lda_model.save(os.path.join(MODELED_DATA_PATH, self.save_name))

        # save corpus
        with open(
            os.path.join(MODELED_DATA_PATH, f"{self.save_name}corpus.pkl"), "wb"
        ) as file:
            pickle.dump(self.corpus, file)

    def get_topics(self, num_words: int = 10) -> list:
        """
        Returns the topics generated by the LDA model.

        parameters:
            num_words (int): Number of words to return for each topic.

        returns:
            (list) Topics generated by the LDA model.
        """
        return self.lda_model.show_topics(
            num_topics=self.optimal_params["num_topics"],
            num_words=num_words,
            formatted=False,
        )

    def get_topics_by_words_df(self) -> None:
        """
        Generates a dataframe of topics and their words.
        """
        topics = self.get_topics()
        df_cols = [
            f"topic_{i}"
            for i in range(1, self.optimal_params["num_topics"] + 1)
        ]
        df_dict = {col: [] for col in df_cols}
        for i, col in enumerate(df_cols):
            for word, _ in topics[i][1]:
                df_dict[col].append(word)

        self.topics_by_words_df = pd.DataFrame(df_dict)

    def get_text_topics_df(self) -> None:
        """
        Generates a dataframe of texts and their topic distributions
        """
        lda_df = pd.DataFrame(
            {
                "title": self.df["title"],
                "topics": [
                    self.lda_model[self.dictionary.doc2bow(tokens)]
                    for tokens in self.tokens
                ],
            }
        )

        # create a column for each topic probability
        topic_prod_dict = {
            i: [0] * len(lda_df)
            for i in range(self.optimal_params["num_topics"])
        }

        # add probabilites
        for index, topicTuples in enumerate(lda_df["topics"]):
            for topicNum, prob in topicTuples:
                topic_prod_dict[topicNum][index] = prob

        # update DataFrame
        for topicNum in range(self.optimal_params["num_topics"]):
            lda_df["topic_{}".format(topicNum)] = topic_prod_dict[topicNum]

        lda_df.drop(columns=["topics"], inplace=True)

        self.topics_by_text_df = lda_df

    def gen_topic_model(self) -> None:
        """
        Main method to generate and evaluate the LDA topic model.
        """
        if not (self.corpus and self.dictionary):
            self.prepare_corpus()

        if not self.lda_model:
            self.random_search()
        else:
            self.optimal_params["num_topics"] = self.lda_model.num_topics
        self.get_topics_by_words_df()
        self.get_text_topics_df()

    def heatmap(self, num_docs: int = 15) -> None:
        """
        Generates a heatmap showing the distribution of topics across documents.

        parameters:
            num_docs (int): Number of documents to include in the heatmap.
        """
        # select a random subset of documents if the dataframe is larger
        # than num_docs
        if len(self.topics_by_text_df) > num_docs:
            subset_df = self.topics_by_text_df.sample(
                n=num_docs, random_state=42
            )
        else:
            subset_df = self.topics_by_text_df

        # extract topic distributions for subset of documents
        subset_corpus = [self.corpus[i] for i in subset_df.index]
        topic_distributions = [
            self.lda_model.get_document_topics(bow, minimum_probability=0)
            for bow in subset_corpus
        ]

        # initialize a dataframe to hold topic distributions for each
        # selected document
        documents = [doc[:30] for doc in subset_df["title"]]
        num_topics = self.lda_model.num_topics
        topic_df = pd.DataFrame(
            data=0,
            index=documents,
            columns=["Topic " + str(i) for i in range(num_topics)],
        )

        # populate dataframe with topic probabilities
        for i, doc_topics in enumerate(topic_distributions):
            for topic, prob in doc_topics:
                topic_df.iloc[i, topic] = prob

        # plot heatmap
        plt.figure(figsize=(12, 8))
        sns.heatmap(
            topic_df, cmap="Reds", cbar_kws={"label": "Topic Probability"}
        )
        plt.xlabel("Topics")
        plt.ylabel("Documents")
        plt.title("Heatmap of Topic Distribution Across Selected Documents")
        plt.xticks(rotation=45, ha="right")
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.show()

    def bar_chart(self, num_words: int = 10) -> None:
        """
        Generates a bar chart showing the importance of words within each topic.

        parameters:
            num_words (int): Number of words to return for each topic.
        """
        topics = self.lda_model.show_topics(
            num_topics=-1, num_words=num_words, formatted=False
        )
        for i, topic in topics:
            plt.figure(figsize=(8, 6))
            topic_words = dict(topic)
            plt.barh(
                range(len(topic_words)),
                list(topic_words.values()),
                align="center",
            )
            plt.yticks(range(len(topic_words)), list(topic_words.keys()))
            plt.xlabel("Word Importance")
            plt.title(f"Topic {i} Word Importance")
            plt.gca().invert_yaxis()
            plt.show()
